{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26210ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from beautifultable import BeautifulTable\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer # Used for stemming\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Add the keras tokenizer for summaries tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # Add padding to help the Keras Sequencing\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy # Loss function being used\n",
    "from sklearn.model_selection import train_test_split # Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9229c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Elastic search\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b306ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\papag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "# Check if the connection was successful\n",
    "if es.ping():\n",
    "    print(\"Connection successful!\")\n",
    "else:\n",
    "    print(\"Connection error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342b19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################################\n",
      "          ___           _     ___                  _      ___           _          \n",
      "  ___ ___| _ ) ___  ___| |__ / __| ___ __ _ _ _ __| |_   | __|_ _  __ _(_)_ _  ___ ©\n",
      " / -_)___| _ \\/ _ \\/ _ \\ / / \\__ \\/ -_) _` | '_/ _| ' \\  | _|| ' \\/ _` | | ' \\/ -_)\n",
      " \\___|   |___/\\___/\\___/_\\_\\ |___/\\___\\__,_|_| \\__|_||_| |___|_||_\\__, |_|_||_\\___|\n",
      "                                                                  |___/            \n",
      "\n",
      "                          __...--~~~~~-._   _.-~~~~~--...__\n",
      "                        //               `V'               \\ \n",
      "                       //                 |                 \\ \n",
      "                      //__...--~~~~~~-._  |  _.-~~~~~~--...__\\ \n",
      "                     //__.....----~~~~._\\ | /_.~~~~----.....__\\\n",
      "                    ====================\\|//====================\n",
      "                                        `---`\n",
      "######################################################################################\n",
      "\n",
      "Enter your ID: 11676\n",
      "Enter a lemma to search: clara\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "######################################################################################\n",
    "          ___           _     ___                  _      ___           _          \n",
    "  ___ ___| _ ) ___  ___| |__ / __| ___ __ _ _ _ __| |_   | __|_ _  __ _(_)_ _  ___ ©\n",
    " / -_)___| _ \\/ _ \\/ _ \\ / / \\__ \\/ -_) _` | '_/ _| ' \\  | _|| ' \\/ _` | | ' \\/ -_)\n",
    " \\___|   |___/\\___/\\___/_\\_\\ |___/\\___\\__,_|_| \\__|_||_| |___|_||_\\__, |_|_||_\\___|\n",
    "                                                                  |___/            \n",
    "\n",
    "                          __...--~~~~~-._   _.-~~~~~--...__\n",
    "                        //               `V'               \\\\ \n",
    "                       //                 |                 \\\\ \n",
    "                      //__...--~~~~~~-._  |  _.-~~~~~~--...__\\\\ \n",
    "                     //__.....----~~~~._\\ | /_.~~~~----.....__\\\\\n",
    "                    ====================\\\\|//====================\n",
    "                                        `---`\n",
    "######################################################################################\n",
    "\"\"\")\n",
    "\n",
    "# User enters his ID and a book lemma to search\n",
    "user_id = int(input(\"Enter your ID: \"))\n",
    "user_search = input(\"Enter a lemma to search: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3606a",
   "metadata": {},
   "source": [
    "### DF with user's ratings for train/test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540676ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe 'rat_df' from 'BX-Book-Ratings.csv' file\n",
    "# Delete all the rows from dataframe which have rating = 0\n",
    "rat_df = pd.read_csv(\"BX-Book-Ratings.csv\")\n",
    "rat_df = rat_df.where(rat_df['rating'] != 0).dropna()\n",
    "\n",
    "# Create pandas dataframe 'book_df' from 'BX-Books.csv' file\n",
    "book_df = pd.read_csv(\"BX-Books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642269cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'avg_rat' dataframe which contains the average rating for each book\n",
    "avg_rat = rat_df.groupby(['isbn'])['rating'].mean().reset_index(name='avg_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c148a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'usr_rat' dataframe which contains all of the user's personal ratings\n",
    "usr_rat = rat_df.where(rat_df['uid'] == user_id).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248a8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'new_df' dataframe by merging 'book_df' and 'usr_rat' dataframes\n",
    "book_df = book_df[['isbn','summary']]\n",
    "new_df = pd.merge(book_df, usr_rat, on='isbn', how='right')\n",
    "new_df = new_df[['summary','uid','rating']]\n",
    "new_df = new_df.where(new_df['summary'] != 0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9939e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-cleaning function\n",
    "def clean_and_reform_data(text):\n",
    "    delete_items = [\"&#39;\", \"&quot;\"]\n",
    "    for item in delete_items:\n",
    "        text = text.replace(item, ' ')\n",
    "    # remove punctuation marks\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9c5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Clean'] = new_df['summary'].apply(clean_and_reform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00aa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_df['WithoutStop'] = new_df['Clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec019f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use English stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply stemming\n",
    "new_df['Stemmed'] = new_df['WithoutStop'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc97ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final processed summaries column from 'new_df' dataframe\n",
    "summaries = new_df[\"Stemmed\"].copy()\n",
    "\n",
    "# Tokenize the summary texts\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(summaries)\n",
    "vocab_size = len(token.word_index) + 1\n",
    "texts = token.texts_to_sequences(summaries) # Integer encode the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fffc81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add zero padding to text sequences\n",
    "texts = pad_sequences(texts, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aec3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af61c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix of one embedding for each unique word in summary texts\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in token.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7fce4",
   "metadata": {},
   "source": [
    "### Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30ed2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing data\n",
    "# Testing data is the 20% of the overall data, and training the 80%\n",
    "textTrain, textTest, ratingTrain, ratingTest = train_test_split(texts, new_df['rating'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a2f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 41, 100)           1094800   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4100)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                262464    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 1,359,707\n",
      "Trainable params: 264,907\n",
      "Non-trainable params: 1,094,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create our neural network model for predicting ratings\n",
    "input_length = texts.shape[1]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=texts.shape[1], trainable=False))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(11, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f93b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2444 samples, validate on 612 samples\n",
      "Epoch 1/100\n",
      "2444/2444 [==============================] - 2s 629us/sample - loss: 2.3270 - accuracy: 0.2209 - val_loss: 2.3067 - val_accuracy: 0.2533\n",
      "Epoch 2/100\n",
      "2444/2444 [==============================] - 1s 287us/sample - loss: 2.2831 - accuracy: 0.2668 - val_loss: 2.3135 - val_accuracy: 0.2271\n",
      "Epoch 3/100\n",
      "2444/2444 [==============================] - 1s 263us/sample - loss: 2.2324 - accuracy: 0.3322 - val_loss: 2.3135 - val_accuracy: 0.2418\n",
      "Epoch 4/100\n",
      "2444/2444 [==============================] - 1s 272us/sample - loss: 2.1818 - accuracy: 0.3858 - val_loss: 2.3313 - val_accuracy: 0.2108\n",
      "Epoch 5/100\n",
      "2444/2444 [==============================] - 1s 280us/sample - loss: 2.1224 - accuracy: 0.4493 - val_loss: 2.3469 - val_accuracy: 0.2042\n",
      "Epoch 6/100\n",
      "2444/2444 [==============================] - 1s 267us/sample - loss: 2.0843 - accuracy: 0.4922 - val_loss: 2.3618 - val_accuracy: 0.1863\n",
      "Epoch 7/100\n",
      "2444/2444 [==============================] - 1s 272us/sample - loss: 2.0562 - accuracy: 0.5180 - val_loss: 2.3516 - val_accuracy: 0.2092\n",
      "Epoch 8/100\n",
      "2444/2444 [==============================] - 1s 275us/sample - loss: 2.0286 - accuracy: 0.5442 - val_loss: 2.3481 - val_accuracy: 0.2092\n",
      "Epoch 9/100\n",
      "2444/2444 [==============================] - 1s 279us/sample - loss: 2.0136 - accuracy: 0.5581 - val_loss: 2.3570 - val_accuracy: 0.2026\n",
      "Epoch 10/100\n",
      "2444/2444 [==============================] - 1s 262us/sample - loss: 1.9939 - accuracy: 0.5822 - val_loss: 2.3696 - val_accuracy: 0.1797\n",
      "Epoch 11/100\n",
      "2444/2444 [==============================] - 1s 259us/sample - loss: 1.9771 - accuracy: 0.6043 - val_loss: 2.3817 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "2444/2444 [==============================] - 1s 265us/sample - loss: 1.9522 - accuracy: 0.6268 - val_loss: 2.3743 - val_accuracy: 0.1814\n",
      "Epoch 13/100\n",
      "2444/2444 [==============================] - 1s 227us/sample - loss: 1.9385 - accuracy: 0.6465 - val_loss: 2.3889 - val_accuracy: 0.1552\n",
      "Epoch 14/100\n",
      "2444/2444 [==============================] - 0s 193us/sample - loss: 1.9160 - accuracy: 0.6702 - val_loss: 2.3933 - val_accuracy: 0.1601\n",
      "Epoch 15/100\n",
      "2444/2444 [==============================] - 1s 260us/sample - loss: 1.8944 - accuracy: 0.6948 - val_loss: 2.3893 - val_accuracy: 0.1732\n",
      "Epoch 16/100\n",
      "2444/2444 [==============================] - 1s 271us/sample - loss: 1.8817 - accuracy: 0.7046 - val_loss: 2.3940 - val_accuracy: 0.1618\n",
      "Epoch 17/100\n",
      "2444/2444 [==============================] - 1s 282us/sample - loss: 1.8707 - accuracy: 0.7152 - val_loss: 2.3956 - val_accuracy: 0.1650\n",
      "Epoch 18/100\n",
      "2444/2444 [==============================] - 1s 272us/sample - loss: 1.8583 - accuracy: 0.7283 - val_loss: 2.3904 - val_accuracy: 0.1699\n",
      "Epoch 19/100\n",
      "2444/2444 [==============================] - 0s 201us/sample - loss: 1.8516 - accuracy: 0.7328 - val_loss: 2.3968 - val_accuracy: 0.1650\n",
      "Epoch 20/100\n",
      "2444/2444 [==============================] - 1s 260us/sample - loss: 1.8411 - accuracy: 0.7475 - val_loss: 2.4061 - val_accuracy: 0.1650\n",
      "Epoch 21/100\n",
      "2444/2444 [==============================] - 1s 272us/sample - loss: 1.8311 - accuracy: 0.7610 - val_loss: 2.4002 - val_accuracy: 0.1716\n",
      "Epoch 22/100\n",
      "2444/2444 [==============================] - 1s 239us/sample - loss: 1.8249 - accuracy: 0.7729 - val_loss: 2.4109 - val_accuracy: 0.1454\n",
      "Epoch 23/100\n",
      "2444/2444 [==============================] - 1s 254us/sample - loss: 1.8115 - accuracy: 0.7844 - val_loss: 2.4156 - val_accuracy: 0.1585\n",
      "Epoch 24/100\n",
      "2444/2444 [==============================] - 1s 283us/sample - loss: 1.8094 - accuracy: 0.7872 - val_loss: 2.4202 - val_accuracy: 0.1471\n",
      "Epoch 25/100\n",
      "2444/2444 [==============================] - 1s 230us/sample - loss: 1.8011 - accuracy: 0.7983 - val_loss: 2.4120 - val_accuracy: 0.1634\n",
      "Epoch 26/100\n",
      "2444/2444 [==============================] - 1s 275us/sample - loss: 1.7917 - accuracy: 0.8118 - val_loss: 2.4159 - val_accuracy: 0.1552\n",
      "Epoch 27/100\n",
      "2444/2444 [==============================] - 1s 229us/sample - loss: 1.7830 - accuracy: 0.8146 - val_loss: 2.4239 - val_accuracy: 0.1552\n",
      "Epoch 28/100\n",
      "2444/2444 [==============================] - 1s 276us/sample - loss: 1.7804 - accuracy: 0.8191 - val_loss: 2.4167 - val_accuracy: 0.1667\n",
      "Epoch 29/100\n",
      "2444/2444 [==============================] - 1s 267us/sample - loss: 1.7683 - accuracy: 0.8343 - val_loss: 2.4245 - val_accuracy: 0.1503\n",
      "Epoch 30/100\n",
      "2444/2444 [==============================] - 1s 246us/sample - loss: 1.7699 - accuracy: 0.8294 - val_loss: 2.4153 - val_accuracy: 0.1667\n",
      "Epoch 31/100\n",
      "2444/2444 [==============================] - 1s 287us/sample - loss: 1.7595 - accuracy: 0.8404 - val_loss: 2.4282 - val_accuracy: 0.1438\n",
      "Epoch 32/100\n",
      "2444/2444 [==============================] - 1s 267us/sample - loss: 1.7588 - accuracy: 0.8388 - val_loss: 2.4319 - val_accuracy: 0.1405\n",
      "Epoch 33/100\n",
      "2444/2444 [==============================] - 1s 234us/sample - loss: 1.7582 - accuracy: 0.8408 - val_loss: 2.4237 - val_accuracy: 0.1585\n",
      "Epoch 34/100\n",
      "2444/2444 [==============================] - 1s 275us/sample - loss: 1.7589 - accuracy: 0.8396 - val_loss: 2.4214 - val_accuracy: 0.1552\n",
      "Epoch 35/100\n",
      "2444/2444 [==============================] - 1s 239us/sample - loss: 1.7548 - accuracy: 0.8445 - val_loss: 2.4300 - val_accuracy: 0.1438\n",
      "Epoch 36/100\n",
      "2444/2444 [==============================] - 1s 259us/sample - loss: 1.7528 - accuracy: 0.8466 - val_loss: 2.4281 - val_accuracy: 0.1569\n",
      "Epoch 37/100\n",
      "2444/2444 [==============================] - 1s 256us/sample - loss: 1.7453 - accuracy: 0.8556 - val_loss: 2.4303 - val_accuracy: 0.1438\n",
      "Epoch 38/100\n",
      "2444/2444 [==============================] - 1s 249us/sample - loss: 1.7478 - accuracy: 0.8556 - val_loss: 2.4237 - val_accuracy: 0.1601\n",
      "Epoch 39/100\n",
      "2444/2444 [==============================] - 1s 249us/sample - loss: 1.7461 - accuracy: 0.8531 - val_loss: 2.4235 - val_accuracy: 0.1552\n",
      "Epoch 40/100\n",
      "2444/2444 [==============================] - 1s 250us/sample - loss: 1.7417 - accuracy: 0.8580 - val_loss: 2.4272 - val_accuracy: 0.1552\n",
      "Epoch 41/100\n",
      "2444/2444 [==============================] - 1s 236us/sample - loss: 1.7453 - accuracy: 0.8552 - val_loss: 2.4281 - val_accuracy: 0.1585\n",
      "Epoch 42/100\n",
      "2444/2444 [==============================] - 1s 273us/sample - loss: 1.7421 - accuracy: 0.8588 - val_loss: 2.4313 - val_accuracy: 0.1471\n",
      "Epoch 43/100\n",
      "2444/2444 [==============================] - 1s 262us/sample - loss: 1.7362 - accuracy: 0.8650 - val_loss: 2.4165 - val_accuracy: 0.1601\n",
      "Epoch 44/100\n",
      "2444/2444 [==============================] - 1s 231us/sample - loss: 1.7353 - accuracy: 0.8650 - val_loss: 2.4169 - val_accuracy: 0.1601\n",
      "Epoch 45/100\n",
      "2444/2444 [==============================] - 1s 278us/sample - loss: 1.7372 - accuracy: 0.8617 - val_loss: 2.4162 - val_accuracy: 0.1650\n",
      "Epoch 46/100\n",
      "2444/2444 [==============================] - 1s 231us/sample - loss: 1.7423 - accuracy: 0.8572 - val_loss: 2.4216 - val_accuracy: 0.1699\n",
      "Epoch 47/100\n",
      "2444/2444 [==============================] - 1s 259us/sample - loss: 1.7422 - accuracy: 0.8539 - val_loss: 2.4130 - val_accuracy: 0.1699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x284c7f20780>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(textTrain, ratingTrain, epochs=100, batch_size=32, validation_split = 0.2,\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a57c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 115us/sample - loss: 2.3904 - accuracy: 0.1924\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(textTest, ratingTest) # Get the loss and accuracy based on the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afbf28",
   "metadata": {},
   "source": [
    "### Get the search results from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "680f7b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\papag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run a query in index 'books' of Elasticsearch\n",
    "res = es.search(index='books', query = {\"match\": {\"book_title\": user_search}}, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f315587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results of the query in 'temp' dictionary\n",
    "temp = {}\n",
    "predict = {}\n",
    "for hit in res['hits']['hits']:\n",
    "    temp[hit['_id']] = [hit['_source']['book_title'], hit['_score']]\n",
    "    predict[hit['_id']] = hit['_source']['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81b61d",
   "metadata": {},
   "source": [
    "### Get the results ready for prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191565e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to pandas dataframe\n",
    "predict_df = pd.DataFrame(predict.items(), columns=['isbn', 'summary'])\n",
    "\n",
    "# Apply text cleaning\n",
    "predict_df['summary'] = predict_df['summary'].apply(clean_and_reform_data)\n",
    "\n",
    "# Remove stopwords\n",
    "predict_df['summary'] = predict_df['summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Apply stemming\n",
    "predict_df['summary'] = predict_df['summary'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b794622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the the summary texts\n",
    "predict = predict_df[\"summary\"].copy()\n",
    "predict_texts = token.texts_to_sequences(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe018f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add zero padding to text sequences\n",
    "predict_texts = pad_sequences(predict_texts, maxlen=input_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3aec7",
   "metadata": {},
   "source": [
    "### Predict personal ratings for the results & proceed with the personalized formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "168806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to 'predict_df' dataframe with the predicted rating from the neural network\n",
    "predict_df['predicted_rating'] = model.predict_classes(predict_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1affe0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\papag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\beautifultable\\utils.py:125: FutureWarning: 'BeautifulTable.column_headers' has been deprecated in 'v1.0.0' and will be removed in 'v1.2.0'. Use 'BTColumnCollection.header' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a table for better visualization of our data\n",
    "table = BeautifulTable(maxwidth=120)\n",
    "table.column_headers = [\"BOOK RESULTS\", \"PERSONALIZED SCORE\", \"BM25 SCORE\", \"AVERAGE RATING\", \"PERSONAL RATING\"]\n",
    "table.set_style(BeautifulTable.STYLE_RST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2794b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for each dictionary 'temp' item\n",
    "for i in temp.keys():\n",
    "    bm25_score = temp.get(i)[1]\n",
    "    if i in avg_rat['isbn'].values: # Check if there is an average rating for the given book\n",
    "        average = avg_rat.loc[avg_rat['isbn'] == i, 'avg_rating'].values[0]\n",
    "    else:\n",
    "        average = 0 # average rating doesn't exist\n",
    "\n",
    "    if i in usr_rat['isbn'].values: # Check if there is a personal rating for the given book\n",
    "        personal = usr_rat.loc[(usr_rat['isbn'] == i), 'rating'].values[0]\n",
    "    else: # personal rating is predicted by the neural network\n",
    "        personal = float(predict_df.loc[(predict_df['isbn'] == i, 'predicted_rating')].values[0])\n",
    "        \n",
    "# ================================= ~ COMPUTE THE PERSONALIZED RANKING FOR EACH CASE ~ ======================================\n",
    "    \n",
    "    # ~ CASE 1 ~\n",
    "    # There is no average rating: personal rating is either positive (>5), or negative (<=5)\n",
    "    if (average == 0):\n",
    "        if personal > 5 :\n",
    "            total_ranking = bm25_score + bm25_score * (1/20) * (personal)\n",
    "        else:\n",
    "            total_ranking = bm25_score - bm25_score/(personal) \n",
    "    \n",
    "    # ~ CASE 2 ~\n",
    "    # There are both average and personal ratings\n",
    "    else:\n",
    "        # Both personal and average ratings are positive (>5)\n",
    "        if (personal > 5) & (average > 5):\n",
    "            total_ranking = bm25_score + bm25_score * ((1/20) * personal + (1/30) * average)\n",
    "        \n",
    "        # Personal rating is positive, but average is negative    \n",
    "        elif (personal > 5) & (average <= 5):\n",
    "            total_ranking = bm25_score + bm25_score * (1/20) * (personal) - bm25_score/(average)\n",
    "        \n",
    "        # Average rating is positive, but personal is negative\n",
    "        elif (personal <= 5) & (average > 5):\n",
    "            total_ranking = bm25_score + bm25_score * (1/30) * (average) - bm25_score/(personal) \n",
    "        \n",
    "        # Both personal and average ratings are negative (<=5)\n",
    "        else:            \n",
    "            total_ranking = bm25_score  - bm25_score/(0.6 * personal + 0.4 * average)\n",
    "    \n",
    "    # Add a new row of data in the table\n",
    "    table.rows.append([temp.get(i)[0], total_ranking, bm25_score, average, personal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1759191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\papag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\beautifultable\\utils.py:125: FutureWarning: 'BeautifulTable.sort' has been deprecated in 'v1.0.0' and will be removed in 'v1.2.0'. Use 'BTRowCollection.sort' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Sort the table by the column 'PERSONALIZED SCORE' in descending order\n",
    "table.sort('PERSONALIZED SCORE', reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba7b1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================== ==================== ============ ================ =================\n",
      "                   BOOK RESULTS                      PERSONALIZED SCORE   BM25 SCORE   AVERAGE RATING   PERSONAL RATING \n",
      "=================================================== ==================== ============ ================ =================\n",
      "                   Clara Callan                            20.935           12.645         7.667              8.0       \n",
      "                  Henry and Clara                          18.014           11.622          7.5               6.0       \n",
      "              Clara Bow: Runnin' Wild                      17.919           10.752          8.0               8.0       \n",
      "          Clara Mondschein's Melancholia                   17.433           11.622           0               10.0       \n",
      "         Sweet Clara and the Freedom Quilt                 16.52            9.351           8.0              10.0       \n",
      "                  Clara : A Novel                          16.271           11.622           0                8.0       \n",
      "              Clara Joins the Circus                       16.128           10.752           0               10.0       \n",
      "        Clara Barton (Women of Achievement)                14.004           10.003           0                8.0       \n",
      "      Driven to Kill: The Clara Harris Story               13.169           8.779            0               10.0       \n",
      " This Old House: The Story of Clara Rust Alaska Pi         12.364           7.419           8.0               8.0       \n",
      "                       oneer                                                                                            \n",
      " Clara Barton : Founder Of The American Red Cross          11.43            6.724           9.0               8.0       \n",
      "          (Childhood Of Famous Americans)                                                                               \n",
      "    Dancing With Clara (Signet Regency Romance)            10.754           9.351           4.0               8.0       \n",
      "    Clara and the Bookwagon (I Can Read Book 3)            9.778            7.823           5.0               9.0       \n",
      " The Glory Cloak : A Novel of Louisa May Alcott an         8.741            6.724            0                6.0       \n",
      "                  d Clara Barton                                                                                        \n",
      " South Bay Trails: Outdoor Adventures in & Around          8.398            5.249           9.0               6.0       \n",
      " Santa Clara Valley : From the Diablo Range to the                                                                      \n",
      "                   Pacific Ocean                                                                                        \n",
      "=================================================== ==================== ============ ================ =================\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe4d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
